{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jyepi\\OneDrive\\Desktop\\FastAICourse\\testProject\\testProjectEnv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "from IPython.display import Audio \n",
    "from scipy.io import wavfile \n",
    "import scipy\n",
    "import scipy.signal\n",
    "import soundfile as sf \n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models \n",
    "from sklearn.metrics import classification_report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./resampledData\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_sample_rate(original_sample_rate, waveform, desired_sample_rate=16000): \n",
    "\tif original_sample_rate != desired_sample_rate: \n",
    "\t\tdesired_length = int( \n",
    "\t\t\tround(float(len(waveform))/original_sample_rate * desired_sample_rate)) \n",
    "\t\twaveform = scipy.signal.resample(waveform, desired_length) \n",
    "\treturn desired_sample_rate, waveform \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(filename): \n",
    "\twav_data, sample_rate = sf.read(file=filename, dtype=np.int16) \n",
    "\tif len(wav_data.shape) > 1: \n",
    "\t\twav_data = np.mean(wav_data, axis=1) \n",
    "\tsample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data) \n",
    "\treturn sample_rate, wav_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = [] \n",
    "for i in os.listdir(data_path): \n",
    "\tfilename = data_path+\"/\"+i \n",
    "\tfilename = filename.format(i=i) \n",
    "\tif os.path.isdir(filename):\n",
    "\t\tfor j in os.listdir(filename): \n",
    "\t\t\tpath = os.path.join(filename, j) \n",
    "\t\t\taudio_data.append([read_audio(path)[1], i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          audio_data     class\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  acoustic\n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  acoustic\n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  acoustic\n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  acoustic\n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  acoustic\n"
     ]
    }
   ],
   "source": [
    "audio_dataframe = pd.DataFrame(audio_data, columns=[\"audio_data\", \"class\"]) \n",
    "print(audio_dataframe.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_length = 80000  # Adjust this length according to your needs; should be five seconds\n",
    "\n",
    "# Function to pad or truncate audio data to the desired length\n",
    "def pad_or_truncate(audio, length):\n",
    "    if len(audio) > length:\n",
    "        return audio[:length]\n",
    "    else:\n",
    "        return np.pad(audio, (0, length - len(audio)), 'constant')\n",
    "\n",
    "# Apply the function to each row in the \"audio_data\" column\n",
    "audio_dataframe['audio_data'] = audio_dataframe['audio_data'].apply(lambda x: pad_or_truncate(x, desired_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = np.array(audio_dataframe[\"audio_data\"].to_list()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jyepi\\OneDrive\\Desktop\\FastAICourse\\testProject\\testProjectEnv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jyepi\\OneDrive\\Desktop\\FastAICourse\\testProject\\testProjectEnv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jyepi\\OneDrive\\Desktop\\FastAICourse\\testProject\\testProjectEnv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jyepi\\OneDrive\\Desktop\\FastAICourse\\testProject\\testProjectEnv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_yamnet = hub.load('https://tfhub.dev/google/yamnet/1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = [] \n",
    "for i in audio_data: \n",
    "    waveform = i / tf.int16.max\n",
    "    scores, embeddings, spectrogram = model_yamnet(waveform) \n",
    "    audio_embeddings.append(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_audio_embeddings = [] \n",
    "for i in audio_embeddings: \n",
    "    padding_needed = 100-i.shape[0] \n",
    "    padded_tensor = tf.pad(i, [[0, padding_needed], [0, 0]]) \n",
    "    padded_audio_embeddings.append(padded_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False) \n",
    "classes = ohe.fit_transform(audio_dataframe[[\"class\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split( \n",
    "    np.array(padded_audio_embeddings), classes, random_state=42, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.3044 - loss: 1.6270\n",
      "Epoch 2/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5540 - loss: 1.3067\n",
      "Epoch 3/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6613 - loss: 1.0446\n",
      "Epoch 4/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7162 - loss: 0.9336\n",
      "Epoch 5/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8080 - loss: 0.6791\n",
      "Epoch 6/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8052 - loss: 0.5798\n",
      "Epoch 7/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7944 - loss: 0.5782\n",
      "Epoch 8/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8109 - loss: 0.5139\n",
      "Epoch 9/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8596 - loss: 0.4176\n",
      "Epoch 10/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8725 - loss: 0.4191\n",
      "Epoch 11/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8671 - loss: 0.3718\n",
      "Epoch 12/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8609 - loss: 0.4050\n",
      "Epoch 13/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9089 - loss: 0.3091\n",
      "Epoch 14/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8797 - loss: 0.3103\n",
      "Epoch 15/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8904 - loss: 0.3314\n",
      "Epoch 16/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9099 - loss: 0.2492\n",
      "Epoch 17/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9124 - loss: 0.2608\n",
      "Epoch 18/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9224 - loss: 0.2426\n",
      "Epoch 19/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8995 - loss: 0.2923\n",
      "Epoch 20/20\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8796 - loss: 0.3120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ed7a3d65d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential([ \n",
    "\tlayers.Input(shape=(100, 1024)), \n",
    "\tlayers.Flatten(), \n",
    "\tlayers.Dense(16, activation='relu'), \n",
    "\tlayers.Dropout(0.1), \n",
    "\tlayers.Dense(16, activation='relu'), \n",
    "\tlayers.Dropout(0.1), \n",
    "\tlayers.Dense(16, activation='relu'), \n",
    "\tlayers.Dense(5, activation='softmax') \n",
    "]) \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "\t\t\tmetrics=['accuracy']) \n",
    "model.fit(xtrain, ytrain, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8869 - loss: 0.3533  \n",
      "0.36590659618377686 0.8794326186180115\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(xtest, ytest) \n",
    "print(loss, accuracy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "[0.12140419 0.17809524 0.5363594  0.11755665 0.04658454]\n",
      "2\n",
      "the audio given is of dog\n"
     ]
    }
   ],
   "source": [
    "def pipeline(filename): \n",
    "\taudio_data = read_audio(filename)[1] \n",
    "\taudio_data = audio_data/tf.int16.max\n",
    "\tscores, embeddings, spectrogram = model_yamnet(audio_data) \n",
    "\tpadding_needed = 100-embeddings.shape[0] \n",
    "\tpadded_tensor = tf.pad(embeddings, [[0, padding_needed], [0, 0]]) \n",
    "\t# Reshape the padded tensor to match the input shape expected by the model \n",
    "\tpadded_tensor = tf.reshape(padded_tensor, (1, 100, 1024)) \n",
    "\tprob = model.predict(padded_tensor)[0] \n",
    "\tprint(prob)\n",
    "\tmax_index = np.argmax(prob) \n",
    "\tprint(max_index)\n",
    "\tif max_index == 0: \n",
    "\t\treturn \"bird\"\n",
    "\telif max_index == 1: \n",
    "\t\treturn \"cat\"\n",
    "\telse: \n",
    "\t\treturn \"dog\"\n",
    "\n",
    "\n",
    "print(\"the audio given is of\", pipeline(r\".\\resampledData\\electric\\Beachside Electric 03.wav\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testProjectEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
